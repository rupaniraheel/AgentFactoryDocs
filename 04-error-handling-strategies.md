---
title: "Error Handling Strategies â€“ Defensive Programming"
chapter: 21
lesson: 4
duration_minutes: 45

# HIDDEN SKILLS METADATA (Institutional Integration Layer)
# Not visible to students; enables competency assessment and differentiation
skills:
  - name: "Strategic Error Recovery"
    proficiency_level: "B1"
    category: "Technical"
    bloom_level: "Apply"
    digcomp_area: "Problem-Solving"
    measurable_at_this_level: "Student applies appropriate error handling strategy (retry, fallback, degradation) to realistic scenario and justifies choice based on error type and context"

  - name: "Real-World Error Analysis"
    proficiency_level: "B1"
    category: "Conceptual"
    bloom_level: "Analyze"
    digcomp_area: "Problem-Solving"
    measurable_at_this_level: "Student identifies multiple error types in realistic scenario and determines which strategy works best for each error"

  - name: "Logging for Debugging"
    proficiency_level: "B1"
    category: "Technical"
    bloom_level: "Apply"
    digcomp_area: "Information Literacy"
    measurable_at_this_level: "Student writes logging statements that capture error context (error type, data, timestamp) useful for debugging without crashing program"

learning_objectives:
  - objective: "Apply retry logic for transient errors (attempt multiple times before failing)"
    proficiency_level: "B1"
    bloom_level: "Apply"
    assessment_method: "Student implements retry pattern with max attempts for file operation or network request"

  - objective: "Implement fallback values for recoverable errors (use default when primary operation fails)"
    proficiency_level: "B1"
    bloom_level: "Apply"
    assessment_method: "Student writes code providing sensible defaults when operation fails"

  - objective: "Use graceful degradation (feature unavailable, but core functionality continues)"
    proficiency_level: "B1"
    bloom_level: "Apply"
    assessment_method: "Student designs system where non-critical failures don't crash entire program"

  - objective: "Choose appropriate error handling strategy for given scenario and explain tradeoffs"
    proficiency_level: "B1"
    bloom_level: "Analyze"
    assessment_method: "Student analyzes scenario, identifies possible errors, and justifies strategy choice"

cognitive_load:
  new_concepts: 2
  assessment: "2 new concepts (Error handling strategies: retry, fallback, graceful degradation, logging; Choosing strategy based on error type) within B1 limit of 10 âœ“. Builds on Lessons 1-3 concepts (try/except, custom exceptions, defensive programming)."

differentiation:
  extension_for_advanced: "Research exponential backoff algorithms; implement jitter in retry logic; explore timeout-based strategies; analyze production logging patterns"
  remedial_for_struggling: "Start with single strategy (fallback) before combining multiple; use real-world scenarios from Lesson 5 capstone as concrete examples"

# Generation metadata
generated_by: "content-implementer v3.0.0"
source_spec: "specs/015-part-4-chapter-23/spec.md"
created: "2025-11-09"
last_modified: "2025-11-09"
git_author: "Claude Code"
workflow: "/sp.implement"
version: "1.0.0"
---

# Error Handling Strategies â€“ Defensive Programming

Catching exceptions prevents crashes. But what happens after you catch an error? How do you recover? This lesson teaches you defensive programming patternsâ€”strategic decisions about how to handle different types of errors. You'll learn when to retry, when to use fallback values, when to degrade gracefully, and how to log errors for debugging.

Professional developers don't just catch errors and move on. They think strategically: **What kind of error is this? Is it temporary? Can I recover? Should I notify the user? What should I log?**

## Beyond Catching â€“ The Four Error Handling Strategies

By now, you've learned try/except/finally blocks and custom exceptions. But catching an exception is just the first step. The real skill is deciding *what to do* when an error occurs.

Let's explore four defensive programming strategies:

### Strategy 1: Retry Logic â€“ "Try Again"

**When to use**: Errors that are **transient** (temporary). Network hiccups, briefly unavailable services, temporary file locks.

**The idea**: If an operation fails, wait a moment and try again. Many transient errors resolve themselves on retry.

**Pattern**:

```python
def fetch_data_with_retry(url: str, max_retries: int = 3) -> str:
    """Fetch data from URL, retry on failure."""
    for attempt in range(1, max_retries + 1):
        try:
            response = fetch_url(url)
            return response
        except ConnectionError as e:
            if attempt == max_retries:
                print(f"Failed after {max_retries} attempts: {e}")
                raise
            print(f"Attempt {attempt} failed, retrying...")
```

**Key idea**: Count attempts. On the last attempt, re-raise the exception so the caller knows it failed completely.

#### ðŸ’¬ AI Colearning Prompt

> "Show me the difference between retry logic and just catching an exception and returning a default value. When would you choose retry vs. fallback?"

#### ðŸŽ“ Expert Insight

> In AI-native development, you don't guess at error handlingâ€”you analyze the error type. Transient errors (network timeouts) demand retry. Permanent errors (file not found) demand fallback or graceful degradation.

### Strategy 2: Fallback Values â€“ "Use a Default"

**When to use**: Errors that are **permanent or unrecoverable**. Missing files, invalid data, unavailable services with no option to retry.

**The idea**: When an operation fails, use a sensible default value instead.

**Pattern**:

```python
def load_config(config_file: str) -> dict:
    """Load configuration, fallback to defaults if missing."""
    try:
        with open(config_file) as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"Config file {config_file} not found, using defaults")
        return {
            "theme": "light",
            "language": "en",
            "notifications": True
        }
    except json.JSONDecodeError:
        print(f"Config file {config_file} is invalid, using defaults")
        return {"theme": "light", "language": "en", "notifications": True}
```

**Key idea**: The program continues with sensible defaults. User gets a notice, but doesn't lose functionality.

#### ðŸš€ CoLearning Challenge

Ask your AI Co-Teacher:
> "For a social media app, if loading the user's profile picture fails, what's a good fallback? Why? If loading the feed fails, is fallback appropriate?"

**Expected Outcome**: You'll understand that some features are essential (feed must load), while others can gracefully fail (profile picture can be placeholder).

### Strategy 3: Graceful Degradation â€“ "Keep Going With Less"

**When to use**: Non-critical features fail, but the system should continue. Feature unavailability doesn't mean complete failure.

**The idea**: When a secondary feature fails, skip it and continue. The user loses functionality but doesn't lose the whole program.

**Pattern**:

```python
def process_csv_with_validation(filename: str) -> list[dict]:
    """Process CSV file, skip invalid rows gracefully."""
    rows = []
    skipped = 0

    try:
        with open(filename) as f:
            reader = csv.DictReader(f)
            for row_num, row in enumerate(reader, start=1):
                try:
                    age = int(row["age"])
                    if age < 0 or age > 150:
                        raise ValueError(f"Age {age} out of range")
                    rows.append(row)
                except ValueError as e:
                    skipped += 1
                    print(f"Row {row_num}: {e}, skipping")
    except FileNotFoundError:
        print(f"File {filename} not found")
        return []

    print(f"Processed {len(rows)} rows, skipped {skipped}")
    return rows
```

**Key idea**: The program doesn't crash on bad data. Invalid rows are skipped with logging. Valid rows are processed. User gets partial results.

#### âœ¨ Teaching Tip

> Use Claude Code to test graceful degradation: "Create a CSV file with 10 rows: 7 valid, 3 with bad data. Run my parser and show what happens. Does it handle all error types?"

### Strategy 4: Logging Errors â€“ "Keep a Record"

**When to use**: All the above strategies benefit from **logging**. Record errors with context for debugging without crashing the program.

**The idea**: Print or record error details: what failed, when, why, and what data was involved. This record helps diagnose issues later.

**Pattern**:

```python
import sys
from datetime import datetime

def divide_with_logging(a: float, b: float) -> float | None:
    """Divide a by b, log errors."""
    try:
        result = a / b
        return result
    except ZeroDivisionError as e:
        timestamp = datetime.now().isoformat()
        error_msg = f"[{timestamp}] ERROR: Division by zero. a={a}, b={b}"
        print(error_msg, file=sys.stderr)  # Print to error stream
        return None
    except TypeError as e:
        timestamp = datetime.now().isoformat()
        error_msg = f"[{timestamp}] ERROR: Invalid types. a={type(a).__name__}, b={type(b).__name__}"
        print(error_msg, file=sys.stderr)
        return None
```

**Key idea**: Log includes timestamp, error type, context (the actual values), and human-readable message. This helps debugging.

#### ðŸ’¬ AI Colearning Prompt

> "What should error logs include to be useful for debugging? What context is essential?"

---

## Code Example 1: Task Validation with Error Handling (PRIMARY EXAMPLE)

Task management systems must validate priority and task existence. Here's how to handle these errors strategically:

```python
class TaskError(Exception):
    """Base exception for task management."""
    pass

class TaskNotFoundError(TaskError):
    """Raised when a task cannot be found by ID."""
    def __init__(self, task_id: int):
        self.task_id = task_id
        super().__init__(f"Task #{task_id} not found")

class InvalidPriorityError(TaskError):
    """Raised when priority is outside valid range."""
    def __init__(self, priority: int, min_val: int = 1, max_val: int = 10):
        self.priority = priority
        super().__init__(f"Priority must be {min_val}-{max_val}, got {priority}")

def validate_task_priority(priority: int) -> None:
    """Validate task priority is in valid range."""
    if not 1 <= priority <= 10:
        raise InvalidPriorityError(priority, min_val=1, max_val=10)

def add_task(tasks: list, title: str, priority: int) -> dict:
    """Add a task with validation."""
    if not title.strip():
        raise ValueError("Task title cannot be empty")
    validate_task_priority(priority)

    task = {
        "id": len(tasks) + 1,
        "title": title,
        "priority": priority,
        "done": False
    }
    tasks.append(task)
    return task

def get_task(tasks: list, task_id: int) -> dict:
    """Retrieve a task by ID, raise error if not found."""
    for task in tasks:
        if task["id"] == task_id:
            return task
    raise TaskNotFoundError(task_id)

# Usage with comprehensive error handling
tasks = []

# Strategy 1: Retry validation for user input
max_attempts = 3
for attempt in range(1, max_attempts + 1):
    try:
        priority = int(input(f"Enter priority (1-10, attempt {attempt}): "))
        validate_task_priority(priority)
        break
    except InvalidPriorityError as e:
        print(f"  Error: {e}")
        if attempt == max_attempts:
            print(f"  Failed after {max_attempts} attempts")
            raise
    except ValueError as e:
        print(f"  Invalid input: {e}")

# Strategy 2: Add task with fallback title
try:
    new_task = add_task(tasks, "", 5)
except ValueError as e:
    print(f"  Error: {e}")
    # Fallback: use default title
    new_task = add_task(tasks, "Untitled Task", 5)
    print(f"  Added with fallback title")

# Strategy 3: Lookup with graceful handling
task_id = 999
try:
    task = get_task(tasks, task_id)
except TaskNotFoundError as e:
    print(f"  Lookup failed: {e} (task ID: {e.task_id})")
    print(f"  Available tasks: {len(tasks)}")
```

**Output**:
```
Error: Priority must be 1-10, got 15
Error: Title cannot be empty
  Added with fallback title
Lookup failed: Task #999 not found (task ID: 999)
Available tasks: 1
```

**Key insight**: This example demonstrates how custom exceptions with context attributes enable smart error recoveryâ€”retry for transient issues, fallback for invalid input, graceful degradation for missing data.

---

## Code Example 2: Retry Logic with Exponential Backoff

Many production systems use **exponential backoff**â€”wait longer between retries. First retry after 1 second, second after 2 seconds, third after 4 seconds. This prevents overwhelming a service.

```python
import time

def fetch_with_exponential_backoff(url: str, max_retries: int = 3) -> str:
    """Fetch data with exponential backoff between retries."""
    for attempt in range(1, max_retries + 1):
        try:
            print(f"Attempt {attempt}: Fetching {url}")
            # Simulate fetch (would use requests.get() in real code)
            if attempt < 2:
                raise ConnectionError("Network timeout")
            return "Success: Data retrieved"
        except ConnectionError as e:
            if attempt == max_retries:
                raise
            wait_time = 2 ** (attempt - 1)  # 1, 2, 4, 8 seconds
            print(f"Failed: {e}. Waiting {wait_time}s before retry {attempt + 1}")
            time.sleep(wait_time)

# Test it
try:
    result = fetch_with_exponential_backoff("https://api.example.com/data")
    print(result)
except ConnectionError:
    print("All retries exhausted")
```

**Output**:
```
Attempt 1: Fetching https://api.example.com/data
Failed: Network timeout. Waiting 1s before retry 2
Attempt 2: Fetching https://api.example.com/data
Failed: Network timeout. Waiting 2s before retry 3
Attempt 3: Fetching https://api.example.com/data
Success: Data retrieved
```

**Specification Reference**: This code demonstrates B1 applicationâ€”student applies retry pattern to realistic network scenario.

---

## Code Example 3: Combining Fallback with Logging

Real code often combines multiple strategies. Here's fallback + logging:

```python
import json
from datetime import datetime

def load_user_preferences(user_id: int) -> dict:
    """Load user preferences, fallback to defaults, log failures."""
    default_prefs = {
        "theme": "light",
        "font_size": 12,
        "notifications": True
    }

    try:
        with open(f"preferences_{user_id}.json") as f:
            data = json.load(f)
            print(f"Loaded preferences for user {user_id}")
            return data
    except FileNotFoundError:
        timestamp = datetime.now().isoformat()
        print(f"[{timestamp}] WARN: Preferences file not found for user {user_id}, using defaults")
        return default_prefs
    except json.JSONDecodeError as e:
        timestamp = datetime.now().isoformat()
        print(f"[{timestamp}] ERROR: Preferences file corrupted for user {user_id}: {e}. Using defaults.")
        return default_prefs
    except Exception as e:
        timestamp = datetime.now().isoformat()
        print(f"[{timestamp}] ERROR: Unexpected error loading preferences for user {user_id}: {e}. Using defaults.")
        return default_prefs
```

**Key details**:
- Catches specific exceptions first (FileNotFoundError, JSONDecodeError)
- Falls back to sensible defaults
- Logs each failure with timestamp and context
- Generic catch-all for unexpected errors
- All errors are loggedâ€”none are silent

---

## Code Example 4: Graceful Degradation in Data Processing

This example processes data, skipping invalid entries:

```python
from dataclasses import dataclass

@dataclass
class User:
    name: str
    age: int
    email: str

def validate_and_process_users(users_data: list[dict]) -> tuple[list[User], list[str]]:
    """Process users, return valid users and list of validation errors."""
    valid_users = []
    errors = []

    for idx, user_dict in enumerate(users_data):
        try:
            # Validate name
            name = user_dict.get("name", "").strip()
            if not name:
                raise ValueError("Name is required")

            # Validate age
            age = int(user_dict["age"])
            if age < 0 or age > 150:
                raise ValueError(f"Age {age} out of valid range")

            # Validate email
            email = user_dict.get("email", "")
            if "@" not in email:
                raise ValueError("Email must contain @")

            valid_users.append(User(name=name, age=age, email=email))

        except (ValueError, KeyError) as e:
            error_msg = f"User {idx + 1}: {e}"
            errors.append(error_msg)
            print(f"WARN: {error_msg}, skipping")

    return valid_users, errors

# Test
test_data = [
    {"name": "Alice", "age": 30, "email": "alice@example.com"},
    {"name": "", "age": 25, "email": "bob@example.com"},  # Invalid: empty name
    {"name": "Charlie", "age": "invalid", "email": "charlie@example.com"},  # Invalid: age not int
    {"name": "Diana", "age": 28, "email": "diana@example.com"},
]

valid, errors = validate_and_process_users(test_data)
print(f"Valid users: {len(valid)}, Errors: {len(errors)}")
for user in valid:
    print(f"  - {user.name} (age {user.age})")
for error in errors:
    print(f"  ERROR: {error}")
```

**Output**:
```
WARN: User 2: Name is required, skipping
WARN: User 3: invalid literal for int() with base 10: 'invalid', skipping
Valid users: 2, Errors: 2
  - Alice (age 30)
  - Diana (age 28)
  ERROR: User 2: Name is required
  ERROR: User 3: invalid literal for int() with base 10: 'invalid'
```

**This demonstrates**:
- Validation happens row-by-row
- Invalid rows are skipped gracefully
- Valid data is processed completely
- Errors are reported without crashing

---

## Code Example 5: Logging with Structured Context

Professional systems log with enough detail to diagnose issues:

```python
import sys
from datetime import datetime
from typing import Any

def safe_json_parse(data: str, context: dict[str, Any]) -> dict | None:
    """Parse JSON with detailed logging."""
    try:
        result = json.loads(data)
        return result
    except json.JSONDecodeError as e:
        timestamp = datetime.now().isoformat()
        log_entry = {
            "timestamp": timestamp,
            "error_type": "JSONDecodeError",
            "error_message": str(e),
            "context": context,
            "data_sample": data[:100] if len(data) > 100 else data
        }
        # In real systems, this would go to a logging service
        print(f"ERROR: {log_entry}", file=sys.stderr)
        return None

# Usage
result = safe_json_parse(
    '{"name": "Alice", invalid json}',
    {"user_id": 42, "operation": "load_profile"}
)
```

**Logging includes**:
- **Timestamp**: When the error occurred
- **Error type**: Specific exception class
- **Error message**: Details from exception
- **Context**: Related information (user ID, operation)
- **Data sample**: What data caused the problem (truncated for safety)

This information helps debuggers understand: **What failed, when, why, and what triggered it.**

---

## Choosing the Right Strategy

Here's a decision matrix for choosing error handling strategies:

| Error Type | Best Strategy | Why | Example |
|-----------|---------------|-----|---------|
| **Transient (temporary)** | Retry | Error resolves on retry | Network timeout, service briefly down |
| **Permanent, predictable** | Fallback | Operation will always fail | File not found, invalid format |
| **Non-critical feature** | Graceful degradation | Core function continues | Thumbnail loading fails, app still works |
| **All errors** | Logging | Track for debugging | Record what happened for diagnosis |

---

## Exercise 1: Implement Retry Logic

Write a function that simulates a network request, fails twice, succeeds on third attempt. Add logging.

```python
def unreliable_fetch(attempt: int = 0) -> str:
    """Simulate unreliable networkâ€”fails twice, succeeds on third."""
    attempt += 1
    if attempt < 3:
        raise ConnectionError(f"Attempt {attempt}: Network timeout")
    return "Data received"

# Your task: Write retry_wrapper() that calls unreliable_fetch()
# with retry logic, logging each attempt, returning success on success
# or raising an exception after 4 failed attempts.

def retry_wrapper(max_attempts: int = 4) -> str:
    """Your implementation here."""
    pass

# Test: Should log 2 failures, then succeed on attempt 3
result = retry_wrapper()
```

**Expected outcome**: Your function retries the operation, logs each attempt, and succeeds when the underlying function succeeds.

---

## Exercise 2: Implement Fallback for Missing Configuration

Write a function that loads configuration from a file, but falls back to defaults if missing or corrupted.

```python
# You have: config_defaults = {"port": 8000, "host": "localhost", "debug": False}
# You need: load_config(filename: str) that:
# - Tries to load JSON from filename
# - Falls back to defaults if FileNotFoundError
# - Falls back to defaults if JSONDecodeError
# - Logs what happened
# - Returns the config (loaded or default)

def load_config(filename: str) -> dict:
    """Your implementation here."""
    pass

# Test with missing file
config = load_config("missing_config.json")
# Should print warning and return defaults
```

**Expected outcome**: Function gracefully handles missing or corrupted files, returning sensible defaults while logging what happened.

---

## Exercise 3: Combine Strategies â€“ Robust Data Processing

Write a function that processes a list of user data, implementing all four strategies:
- **Retry**: Attempts validation multiple times (in case data quality issue is temporary)
- **Fallback**: Uses default values for missing fields
- **Graceful degradation**: Skips invalid rows, continues processing others
- **Logging**: Records errors with context

```python
def process_user_data(users: list[dict], max_validation_attempts: int = 2) -> list[dict]:
    """Process users with retry, fallback, graceful degradation, logging."""
    processed = []

    for idx, user in enumerate(users):
        # Implement retry loop: attempt validation up to max_validation_attempts times
        # Implement fallback: use defaults for missing fields
        # Implement graceful degradation: skip invalid users
        # Implement logging: record errors
        pass

    return processed

# Test data
test_users = [
    {"name": "Alice", "age": 30, "email": "alice@example.com"},
    {"name": "Bob"},  # Missing age and email
    {"name": "Charlie", "age": "invalid"},  # Invalid age
]

result = process_user_data(test_users)
```

**Expected outcome**: Function processes valid data, skips invalid, uses defaults where possible, logs all issues.

---

## Key Takeaway: Defensive Programming Mindset

Professional developers ask:

1. **What errors are possible?** (What exceptions could this raise?)
2. **Are they transient or permanent?** (Will retry help?)
3. **What's the right recovery strategy?** (Retry, fallback, degrade?)
4. **What should I log?** (What context helps debugging?)

This systematic thinking transforms error handling from "catch and hope" to "anticipate and recover."

---

## Try With AI

Master strategic error handling through decision frameworks for production systems.

**ðŸ” Explore Strategy Selection:**
> "Explain the four error handling strategies: retry (transient errors), fallback (use defaults), graceful degradation (feature unavailable, core works), and fail-fast (unrecoverable). Show weather app examples for API fetch (critical), cached history (optional), location upload (background), and profile picture (UI)."

**ðŸŽ¯ Practice Error Classification:**
> "I have a weather app with 4 operations. For each, identify: what errors occur (API timeout, file missing, network failure), are they transient or permanent, which strategy fits best, and what to log. Walk me through decision criteria for each operation."

**ðŸ§ª Test Strategy Edge Cases:**
> "Analyze three scenarios: (A) Database connection retry with exponential backoff, (B) Corrupted image upload retry, (C) Silent fallback for corrupted JSON preferences. For each, critique my strategy choice and explain when retry vs fail-fast vs fallback is appropriate."

**ðŸš€ Apply Production Decision Framework:**
> "Build `error_strategies.py` with four decorator functions: `@retry_on_failure(max_attempts=3, backoff=exponential)`, `@fallback_on_error(default_value=None)`, `@degrade_gracefully(feature_name)`, and `@log_errors(log_level)`. Include decision tree for error classification (transient vs permanent, critical vs non-critical) and logging best practices."

---

**Safety and Responsible Error Handling**

As you implement error handling, remember:

- **Log securely**: Don't log passwords, API keys, or personal data
- **User-friendly messages**: Tell users what went wrong and what to do (not internal error codes)
- **Graceful failure**: Never silently ignore errorsâ€”log and inform
- **Test error paths**: Intentionally trigger errors to verify recovery works
